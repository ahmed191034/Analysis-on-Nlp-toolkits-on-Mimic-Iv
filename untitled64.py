# -*- coding: utf-8 -*-
"""Untitled64.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12ed2ABwXJnGDCiffcakndzgjrSxKTha0
"""

!pip install torch transformers scikit-learn pandas
import pandas as pd
import re
from sklearn.metrics import precision_score, recall_score, f1_score
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from difflib import SequenceMatcher
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/discharge_sample.csv")
df.head()

df.shape

# Medical abbreviation dictionary
ABBR_DICT = {
    'y/o': 'year old', 'yo': 'year old', 'wk': 'week', 'mo': 'month', 'yr': 'year',
    's/p': 'status post', 'hx': 'history', 'h/o': 'history of', 'pmh': 'past medical history',
    'hpi': 'history of present illness', 'ros': 'review of systems', 'n/v/d': 'nausea vomiting and diarrhea',
    'n/v': 'nausea and vomiting', 'c/o': 'complains of', 'c/w': 'consistent with',
    'sob': 'shortness of breath', 'htn': 'hypertension', 'dm': 'diabetes mellitus',
    'cad': 'coronary artery disease', 'copd': 'chronic obstructive pulmonary disease',
    'mi': 'myocardial infarction', 'chf': 'congestive heart failure', 'pna': 'pneumonia',
    'uti': 'urinary tract infection', 'gi': 'gastrointestinal', 'ct': 'computed tomography',
    'mri': 'magnetic resonance imaging', 'rrr': 'regular rate and rhythm', 'iv': 'intravenous',
    'po': 'by mouth', 'abx': 'antibiotics', 'fx': 'fracture', 'prn': 'as needed'
}

# Precompile regex
ABBR_PATTERNS = {re.compile(r'\b' + re.escape(k) + r'\b'): v for k, v in ABBR_DICT.items()}

def clean_text(text: str) -> str:
    """Expand abbreviations, normalize whitespace, and remove symbols."""
    if not isinstance(text, str):
        return ""
    s = text.lower()
    s = re.sub(r'_+', ' ', s)
    for pattern, full in ABBR_PATTERNS.items():
        s = pattern.sub(full, s)
    s = re.sub(r'(\d+)wk\b', r'\1 week', s)
    s = re.sub(r'(\d+)mo\b', r'\1 month', s)
    s = re.sub(r'(\d+)yr\b', r'\1 year', s)
    s = re.sub(r'[^a-zA-Z0-9\\. ]+', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

# Apply cleaning
df["cleaned_text"] = df["text"].apply(clean_text)
df.head(2)

model_name = "samrawal/bert-base-uncased_clinical-ner"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

nlp_clinicalbert = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")
print("âœ… ClinicalBERT model loaded successfully!")

def extract_entities_clinicalbert(text):
    if not isinstance(text, str) or text.strip() == "":
        return {"Pred_Disease": "", "Pred_Procedure": "", "Pred_Treatment": ""}

    results = nlp_clinicalbert(text[:1000])  # limit for speed
    diseases, treatments, procedures = set(), set(), set()

    for ent in results:
        label = ent["entity_group"].upper()
        word = re.sub(r"[^a-zA-Z0-9\\s\\-]", "", ent["word"]).strip()
        if not word:
            continue
        if label in ["PROBLEM", "DISEASE", "SYMPTOM", "DISORDER"]:
            diseases.add(word)
        elif label in ["TREATMENT", "DRUG", "THERAPY", "MEDICATION"]:
            treatments.add(word)
        elif label in ["TEST", "PROCEDURE", "OPERATION"]:
            procedures.add(word)

    return {
        "Pred_Disease": ", ".join(sorted(diseases)),
        "Pred_Procedure": ", ".join(sorted(procedures)),
        "Pred_Treatment": ", ".join(sorted(treatments))
    }

# Apply extraction
results = df["cleaned_text"].apply(extract_entities_clinicalbert)
results_df = pd.DataFrame(list(results))
df_final = pd.concat([df, results_df], axis=1)

df_final.to_csv("/content/drive/MyDrive/clinicalbert_predictions.csv", index=False)
print("âœ… Predictions saved to Drive!")
df_final.head(3)

import pandas as pd
from IPython.display import display, HTML

# (Re-run this only if you haven't already)
pred_df = pd.read_csv("/content/drive/MyDrive/clinicalbert_predictions.csv")
true_df = pd.read_csv("/content/drive/MyDrive/ground_truth_annotations.csv")
merged_df = pd.merge(pred_df, true_df, on="note_id", how="inner")

# Normalize helper
def normalize_entities(text):
    if pd.isna(text): return []
    return [t.strip().lower() for t in str(text).replace(';', ',').split(',') if t.strip()]

def entity_metrics(row, pred_col, true_col):
    pred_set = set(normalize_entities(row[pred_col]))
    true_set = set(normalize_entities(row[true_col]))
    if not pred_set and not true_set: return 1
    if not pred_set or not true_set: return 0
    tp = len(pred_set & true_set)
    fp = len(pred_set - true_set)
    fn = len(true_set - pred_set)
    precision = tp / (tp + fp) if (tp + fp) else 0
    recall = tp / (tp + fn) if (tp + fn) else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0
    return round(f1, 3)

# Compute per-row F1s
merged_df["Disease_F1"] = merged_df.apply(lambda r: entity_metrics(r, "Pred_Disease", "Actual_Disease"), axis=1)
merged_df["Treatment_F1"] = merged_df.apply(lambda r: entity_metrics(r, "Pred_Treatment", "Actual_Treatment"), axis=1)

# Select & rename columns for presentation
table_df = merged_df[[
    "note_id", "Pred_Disease", "Actual_Disease", "Disease_F1",
    "Pred_Treatment", "Actual_Treatment", "Treatment_F1"
]].rename(columns={
    "note_id": "Note ID",
    "Pred_Disease": "Predicted Disease(s)",
    "Actual_Disease": "Actual Disease(s)",
    "Disease_F1": "Disease F1",
    "Pred_Treatment": "Predicted Treatment(s)",
    "Actual_Treatment": "Actual Treatment(s)",
    "Treatment_F1": "Treatment F1"
})

# Style table like in your reference figure
styled = (
    table_df
    .style
    .set_table_styles([
        {'selector': 'th', 'props': [('background-color', '#f2f2f2'),
                                     ('font-weight', 'bold'),
                                     ('text-align', 'center')]},
        {'selector': 'td', 'props': [('text-align', 'left'),
                                     ('border', '1px solid #ccc'),
                                     ('padding', '5px')]}
    ])
    .highlight_between(subset=["Disease F1", "Treatment F1"], left=0.5, right=1.0, color="#d4edda")
    .set_properties(**{'border-collapse': 'collapse', 'width': '100%'})
)

display(HTML("<h3>Per-Note Prediction vs Actual Comparison</h3>"))
display(styled)

from IPython.display import HTML, display

html_table = merged_df[[
    "note_id", "Pred_Disease", "Actual_Disease", "Disease_F1",
    "Pred_Treatment", "Actual_Treatment", "Treatment_F1"
]].rename(columns={
    "note_id": "Note ID",
    "Pred_Disease": "Predicted Disease(s)",
    "Actual_Disease": "Actual Disease(s)",
    "Disease_F1": "Disease F1",
    "Pred_Treatment": "Predicted Treatment(s)",
    "Actual_Treatment": "Actual Treatment(s)",
    "Treatment_F1": "Treatment F1"
}).to_html(index=False, justify="center", escape=False)

display(HTML(f"<h3>ðŸ©º Per-Note Prediction vs Actual Comparison (ClinicalBERT)</h3>{html_table}"))

import torch, numpy as np, stanza
from torch.serialization import add_safe_globals

# âœ… Allow older pickled tensor formats
add_safe_globals([np.ndarray, np.core.multiarray._reconstruct])

# âœ… Monkey-patch torch.load safely for stanza (trusted source)
old_load = torch.load
def patched_torch_load(*args, **kwargs):
    kwargs["weights_only"] = False
    return old_load(*args, **kwargs)
torch.load = patched_torch_load

# âœ… Download i2b2 + default tokenizer
stanza.download("en", package="i2b2")
stanza.download("en")

# âœ… Build full clinical pipeline
nlp_stanza = stanza.Pipeline(
    lang="en",
    processors="tokenize,ner",
    package={"tokenize": "default", "ner": "i2b2"},
    use_gpu=True
)

print("âœ… SUCCESS: Stanza (i2b2) clinical NER model fully loaded and ready for inference!")

# Extract entities for each note
stanza_results = []

for i, text in enumerate(df["cleaned_text"]):
    doc = nlp_stanza(text)
    diseases, treatments, procedures = [], [], []

    for ent in doc.ents:
        label = ent.type.upper()
        if label in ["PROBLEM", "DISEASE", "DIAGNOSIS"]:
            diseases.append(ent.text)
        elif label in ["TREATMENT", "MEDICATION", "DRUG"]:
            treatments.append(ent.text)
        elif label in ["TEST", "PROCEDURE", "SURGERY"]:
            procedures.append(ent.text)

    stanza_results.append({
        "Pred_Disease": ", ".join(set(diseases)),
        "Pred_Procedure": ", ".join(set(procedures)),
        "Pred_Treatment": ", ".join(set(treatments))
    })

# Merge results with dataset
df_stanza = pd.concat([df, pd.DataFrame(stanza_results)], axis=1)

print("âœ… Entity extraction completed for all rows!")
df_stanza[["text", "Pred_Disease", "Pred_Procedure", "Pred_Treatment"]].head(3)

merged_df.to_excel(
    "/content/drive/MyDrive/PerNote_Comparison.xlsx",
    index=False,
    sheet_name="Comparison"
)
print("âœ… Saved as Excel: PerNote_Comparison.xlsx")

