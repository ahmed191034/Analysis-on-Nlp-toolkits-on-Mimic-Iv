# -*- coding: utf-8 -*-
"""Evaluationipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19JEYKAgoiAtGUwXe0kfa3aG0RFb74uoz
"""

from collections import defaultdict
from typing import List, Tuple, Dict

Span = Tuple[int, int]     # (start, end) offsets
LabeledSpan = Tuple[int, int, str]  # (start, end, label)

def overlap(span_a: Span, span_b: Span) -> bool:
    """Return True if two spans overlap (lenient match)."""
    return max(span_a[0], span_b[0]) < min(span_a[1], span_b[1])

def exact_match(pred: LabeledSpan, gold: LabeledSpan, strict: bool = True) -> bool:
    """Return True if two labeled spans match exactly.

    If strict=True, both span and label must match; if strict=False, only span boundaries must match.
    """
    same_span = (pred[0] == gold[0] and pred[1] == gold[1])
    return same_span and (pred[2] == gold[2] if strict else True)

def evaluate_spans(
    gold_spans: List[LabeledSpan],
    pred_spans: List[LabeledSpan],
    match_func,
    labels: List[str] = None,
) -> Dict[str, Dict[str, float]]:
    """
    Generic evaluation function.

    Parameters
    ----------
    gold_spans: list of gold (start, end, label)
    pred_spans: list of predicted (start, end, label)
    match_func: function that takes (pred_span, gold_span) and returns True if they match
    labels: optional list of labels to compute perâ€‘label metrics

    Returns
    -------
    A dict mapping each label and 'overall' to a dict with 'precision', 'recall' and 'f1'
    """
    if labels is None:
        labels = sorted({lbl for _,_,lbl in gold_spans} | {lbl for _,_,lbl in pred_spans})
    metrics = {}
    # compute perâ€‘label counts
    for label in labels:
        # select spans of that label
        gold_lbl = [s for s in gold_spans if s[2] == label]
        pred_lbl = [s for s in pred_spans if s[2] == label]
        # mark gold spans as matched
        matched_gold = set()
        tp = 0
        for p in pred_lbl:
            matched = False
            for i, g in enumerate(gold_lbl):
                if i not in matched_gold and match_func(p, g):
                    matched = True
                    matched_gold.add(i)
                    break
            if matched:
                tp += 1
        fp = len(pred_lbl) - tp
        fn = len(gold_lbl) - tp
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        metrics[label] = {
            'tp': tp, 'fp': fp, 'fn': fn,
            'precision': precision, 'recall': recall, 'f1': f1
        }
    # compute microâ€‘averaged metrics
    total_tp = sum(m['tp'] for m in metrics.values())
    total_fp = sum(m['fp'] for m in metrics.values())
    total_fn = sum(m['fn'] for m in metrics.values())
    p = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    r = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    f = 2 * p * r / (p + r) if (p + r) > 0 else 0.0
    metrics['overall'] = {'precision': p, 'recall': r, 'f1': f}
    return metrics

def strict_evaluation(gold_spans: List[LabeledSpan], pred_spans: List[LabeledSpan], strict: bool = True):
    return evaluate_spans(
        gold_spans,
        pred_spans,
        lambda p, g: exact_match(p, g, strict=strict),
    )

def partial_evaluation(gold_spans: List[LabeledSpan], pred_spans: List[LabeledSpan], match_type: bool = False):
    def match_func(pred_span: LabeledSpan, gold_span: LabeledSpan) -> bool:
        spans_overlap = overlap((pred_span[0], pred_span[1]), (gold_span[0], gold_span[1]))
        if not spans_overlap:
            return False
        return pred_span[2] == gold_span[2] if match_type else True
    return evaluate_spans(gold_spans, pred_spans, match_func)

import pandas as pd

# load predictions and ground truth
pred_df = pd.read_csv('/content/drive/MyDrive/clinicalbert_predictions.csv')
gold_df = pd.read_csv('/content/drive/MyDrive/ground_truth_annotations.csv')

def get_entities(row, col_name):
    """Return a list of entity strings (split by commas) or an empty list."""
    val = row.get(col_name)
    if pd.isna(val):
        return []
    return [e.strip().lower() for e in str(val).split(',') if e.strip()]

metrics_strict, metrics_lenient = [], []

for _, pred_row in pred_df.iterrows():
    note_id = pred_row['note_id']
    gold_row = gold_df[gold_df['note_id'] == note_id].iloc[0]
    # assemble gold and predicted entities for each type
    for label, pred_col, gold_col in [
        ('disease', 'Pred_Disease', 'Actual_Disease'),
        ('procedure', 'Pred_Procedure', 'Actual_Treatment'),
        ('treatment', 'Pred_Treatment', 'Actual_Treatment'),
    ]:
        pred_entities = get_entities(pred_row, pred_col)
        gold_entities = get_entities(gold_row, gold_col)
        # convert to pseudoâ€‘span tuples; we do not compute true offsets here
        pred_spans = [(0, 1, label) for _ in pred_entities]  # placeholder spans
        gold_spans = [(0, 1, label) for _ in gold_entities]
        # strict (exact) evaluation: exact span and type match
        strict_result = strict_evaluation(gold_spans, pred_spans, strict=True)
        # lenient evaluation: partial overlap, ignore type
        lenient_result = partial_evaluation(gold_spans, pred_spans, match_type=False)
        metrics_strict.append(strict_result['overall'])
        metrics_lenient.append(lenient_result['overall'])

# aggregate across notes (microâ€‘average is already aggregated)
overall_strict_precision = sum(m['precision'] for m in metrics_strict) / len(metrics_strict)
overall_strict_recall    = sum(m['recall']    for m in metrics_strict) / len(metrics_strict)
overall_strict_f1        = sum(m['f1']        for m in metrics_strict) / len(metrics_strict)

print('Average strict precision:', overall_strict_precision)
print('Average strict recall:',    overall_strict_recall)
print('Average strict F1:',        overall_strict_f1)

def cui_match(pred, gold):
    # match if spans overlap and CUIs match
    spans_overlap = overlap((pred[0], pred[1]), (gold[0], gold[1]))
    return spans_overlap and pred[3] == gold[3]

def evaluate_concepts(gold_spans, pred_spans):
    return evaluate_spans(gold_spans, pred_spans, cui_match)

# ============================================
# ðŸ§  Step 1: Import libraries and define helper functions
# ============================================
import pandas as pd
from typing import List, Tuple, Dict

Span = Tuple[int, int]
LabeledSpan = Tuple[int, int, str]

def overlap(span_a: Span, span_b: Span) -> bool:
    return max(span_a[0], span_b[0]) < min(span_a[1], span_b[1])

def exact_match(pred: LabeledSpan, gold: LabeledSpan, strict: bool = True) -> bool:
    same_span = (pred[0] == gold[0] and pred[1] == gold[1])
    return same_span and (pred[2] == gold[2] if strict else True)

def evaluate_spans(
    gold_spans: List[LabeledSpan],
    pred_spans: List[LabeledSpan],
    match_func,
    labels: List[str] = None,
) -> Dict[str, Dict[str, float]]:
    if labels is None:
        labels = sorted({lbl for _, _, lbl in gold_spans} | {lbl for _, _, lbl in pred_spans})
    metrics = {}
    for label in labels:
        gold_lbl = [s for s in gold_spans if s[2] == label]
        pred_lbl = [s for s in pred_spans if s[2] == label]
        matched_gold = set()
        tp = 0
        for p in pred_lbl:
            for i, g in enumerate(gold_lbl):
                if i not in matched_gold and match_func(p, g):
                    tp += 1
                    matched_gold.add(i)
                    break
        fp = len(pred_lbl) - tp
        fn = len(gold_lbl) - tp
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        metrics[label] = {'tp': tp, 'fp': fp, 'fn': fn,
                          'precision': precision, 'recall': recall, 'f1': f1}
    total_tp = sum(m['tp'] for m in metrics.values())
    total_fp = sum(m['fp'] for m in metrics.values())
    total_fn = sum(m['fn'] for m in metrics.values())
    p = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    r = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    f = 2 * p * r / (p + r) if (p + r) > 0 else 0.0
    metrics['overall'] = {'precision': p, 'recall': r, 'f1': f}
    return metrics

def strict_evaluation(gold_spans, pred_spans, strict=True):
    return evaluate_spans(gold_spans, pred_spans, lambda p, g: exact_match(p, g, strict=strict))

def partial_evaluation(gold_spans, pred_spans, match_type=False):
    def match_func(pred_span, gold_span):
        spans_overlap = overlap((pred_span[0], pred_span[1]), (gold_span[0], gold_span[1]))
        if not spans_overlap:
            return False
        return pred_span[2] == gold_span[2] if match_type else True
    return evaluate_spans(gold_spans, pred_spans, match_func)

# ============================================
# ðŸ“„ Step 2: Load your Stanza and Gold CSVs
# ============================================
pred_df = pd.read_csv('/content/drive/MyDrive/stanza_predictions.csv')
gold_df = pd.read_csv('/content/drive/MyDrive/ground_truth_annotations.csv')

# check column names for debugging
print("âœ… Columns in stanza_predictions.csv:", list(pred_df.columns))
print("âœ… Columns in ground_truth_annotations.csv:", list(gold_df.columns))

# ============================================
# ðŸ§© Step 3: Extract entities safely
# ============================================
def get_entities(row, col_name):
    if col_name not in row or pd.isna(row[col_name]):
        return []
    return [e.strip().lower() for e in str(row[col_name]).split(',') if e.strip()]

metrics_strict, metrics_lenient = [], []

for _, pred_row in pred_df.iterrows():
    note_id = pred_row['note_id']
    gold_row = gold_df[gold_df['note_id'] == note_id]
    if gold_row.empty:
        continue  # skip if gold annotation not found
    gold_row = gold_row.iloc[0]

    for label, pred_col, gold_col in [
        ('disease', 'Pred_Disease', 'Actual_Disease'),
        ('procedure', 'Pred_Procedure', 'Actual_Treatment'),
        ('treatment', 'Pred_Treatment', 'Actual_Treatment'),
    ]:
        if pred_col not in pred_df.columns or gold_col not in gold_df.columns:
            continue  # skip missing columns

        pred_entities = get_entities(pred_row, pred_col)
        gold_entities = get_entities(gold_row, gold_col)

        pred_spans = [(0, 1, label) for _ in pred_entities]
        gold_spans = [(0, 1, label) for _ in gold_entities]

        strict_result = strict_evaluation(gold_spans, pred_spans)
        lenient_result = partial_evaluation(gold_spans, pred_spans)

        metrics_strict.append(strict_result['overall'])
        metrics_lenient.append(lenient_result['overall'])

# ============================================
# ðŸ“Š Step 4: Compute final averages
# ============================================
def avg_metric(metrics_list, key):
    return sum(m[key] for m in metrics_list) / len(metrics_list) if metrics_list else 0.0

strict_p = avg_metric(metrics_strict, 'precision')
strict_r = avg_metric(metrics_strict, 'recall')
strict_f = avg_metric(metrics_strict, 'f1')

lenient_p = avg_metric(metrics_lenient, 'precision')
lenient_r = avg_metric(metrics_lenient, 'recall')
lenient_f = avg_metric(metrics_lenient, 'f1')

print("\nðŸ“ˆ === STANZA EVALUATION RESULTS ===")
print(f"Strict Precision: {strict_p:.4f}")
print(f"Strict Recall:    {strict_r:.4f}")
print(f"Strict F1:        {strict_f:.4f}")
print(f"Lenient Precision:{lenient_p:.4f}")
print(f"Lenient Recall:   {lenient_r:.4f}")
print(f"Lenient F1:       {lenient_f:.4f}")

import pandas as pd
from IPython.display import display, HTML

# Load both models' predictions
cb = pd.read_csv('/content/drive/MyDrive/clinicalbert_predictions.csv')
st = pd.read_csv('/content/drive/MyDrive/stanza_predictions.csv')

# Merge them for side-by-side comparison
compare_df = cb[['note_id', 'Pred_Disease', 'Pred_Treatment']].merge(
    st[['note_id', 'Pred_Disease', 'Pred_Treatment']],
    on='note_id', suffixes=('_ClinicalBERT', '_Stanza')
)

# Create match indicators
compare_df['Disease_Match'] = compare_df['Pred_Disease_ClinicalBERT'] == compare_df['Pred_Disease_Stanza']
compare_df['Treatment_Match'] = compare_df['Pred_Treatment_ClinicalBERT'] == compare_df['Pred_Treatment_Stanza']

# Reorder columns for clarity
compare_df = compare_df[['note_id', 'Disease_Match', 'Treatment_Match',
                         'Pred_Disease_ClinicalBERT', 'Pred_Disease_Stanza',
                         'Pred_Treatment_ClinicalBERT', 'Pred_Treatment_Stanza']]

# âœ… Dark-mode friendly color scheme
def highlight_match(val):
    if val is True:
        return 'background-color: #81C784; color: black; font-weight: bold;'  # medium green
    elif val is False:
        return 'background-color: #E57373; color: black; font-weight: bold;'  # red
    else:
        return ''

# Display styled table
pd.set_option('display.max_colwidth', 120)
styled = compare_df.head(10).style.map(highlight_match, subset=['Disease_Match', 'Treatment_Match'])
display(styled)

# âœ… Optional â€” save table as HTML for your report
html_path = '/content/drive/MyDrive/ClinicalBERT_vs_Stanza_Comparison.html'
styled.to_html(html_path)
print(f"\nâœ… Table saved as HTML: {html_path}")

# âœ… Optional â€” save CSV version (without color)
csv_path = '/content/drive/MyDrive/ClinicalBERT_vs_Stanza_Comparison.csv'
compare_df.to_csv(csv_path, index=False)
print(f"âœ… Plain CSV saved: {csv_path}")

